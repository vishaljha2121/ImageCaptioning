{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('AI': conda)"
  },
  "interpreter": {
   "hash": "0c4132d5ede56bd40b0bdd125398cfd152a99577fc6ec03da27af8e648161214"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IMPORTING LIBRARIES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "file_location_images = '/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/Dataset/Flickr_Data/Flickr_Data/Images/'\n",
    "len(file_location_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "os.environ[\"RUNFILES_DIR\"] = \"/Library/Frameworks/Python.framework/Versions/3.7/share/plaidml\"\n",
    "# plaidml might exist in different location. Look for \"/usr/local/share/plaidml\" and replace in above path\n",
    "\n",
    "os.environ[\"PLAIDML_NATIVE_PATH\"] = \"/Library/Frameworks/Python.framework/Versions/3.7/lib/libplaidml.dylib\"\n",
    "# libplaidml.dylib might exist in different location. Look for \"/usr/local/lib/libplaidml.dylib\" and replace in above path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.preprocessing import image, sequence\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, concatenate, Activation, Flatten, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.layers import Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location_caption = '/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/Dataset/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'\n",
    "file_location_images = '/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/Dataset/Flickr_Data/Flickr_Data/Images/'\n",
    "\n",
    "file_location_train_images = '/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/Dataset/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt'\n",
    "file_location_val_images = '/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/Dataset/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.devImages.txt'\n",
    "file_location_test_images = '/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/Dataset/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.testImages.txt'\n",
    "\n",
    "file_location_training_data = '/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/Dataset/Flickr_Data/Flickr_Data/flickr8ktextfiles/flickr_8k_train_dataset.txt'"
   ]
  },
  {
   "source": [
    "# GENERATING A DICTIONARY OF CAPTIONS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = open(file_location_caption, 'r').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_captions = {}\n",
    "for i, row in enumerate(captions):\n",
    "    row = row.split('\\t')\n",
    "    row[0] = row[0][:len(row[0])-2]\n",
    "    if row[0] in dictionary_captions:\n",
    "        dictionary_captions[row[0]].append(row[1])\n",
    "    else:\n",
    "        dictionary_captions[row[0]] = [row[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"A couple stands close at the water 's edge .\",\n",
       " 'The two people stand by a body of water and in front of bushes in fall .',\n",
       " 'Two people hold each other near a pond .',\n",
       " 'Two people stand by the water .',\n",
       " 'Two people stand together on the edge of the water on the grass .']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "dictionary_captions['3637013_c675de7705.jpg']"
   ]
  },
  {
   "source": [
    "# GETTING ALL TRAINING, VALIDATION, TESTING IMAGES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/Dataset/Flickr_Data/Flickr_Data/Images/2387197355_237f6f41ee.jpg',\n",
       " '/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/Dataset/Flickr_Data/Flickr_Data/Images/2609847254_0ec40c1cce.jpg',\n",
       " '/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/Dataset/Flickr_Data/Flickr_Data/Images/2046222127_a6f300e202.jpg',\n",
       " '/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/Dataset/Flickr_Data/Flickr_Data/Images/2853743795_e90ebc669d.jpg',\n",
       " '/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/Dataset/Flickr_Data/Flickr_Data/Images/2696951725_e0ae54f6da.jpg']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "global img \n",
    "img = glob.glob(file_location_images+'*.jpg')\n",
    "img[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_training = set(open(file_location_train_images, 'r').read().strip().split('\\n'))\n",
    "images_validation = set(open(file_location_val_images, 'r').read().strip().split('\\n')) \n",
    "images_testing = set(open(file_location_test_images, 'r').read().strip().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_data(l):\n",
    "    temp = []\n",
    "    for i in img:\n",
    "        if i[len(file_location_images):] in l:\n",
    "            temp.append(i)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = split_image_data(images_training)\n",
    "val = split_image_data(images_validation)\n",
    "test = split_image_data(images_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6000, 1000, 1000)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "len(train), len(val), len(test)"
   ]
  },
  {
   "source": [
    "# PREPROCESSING IMAGES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(x):\n",
    "    x /= 255.\n",
    "    x -= 0.5\n",
    "    x += 2.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_data(image_path):\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "source": [
    "# LOAD VGG-16 MODEL TRAINED ON IMAGE-NET DATASET"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionV3(weights='imagenet')\n",
    "\n",
    "new_input = model.input\n",
    "hidden_layer = model.layers[-2].output\n",
    "\n",
    "model_new = Model(new_input, hidden_layer)"
   ]
  },
  {
   "source": [
    "# ENCODING ALL DATASET USING ALTERED VGG-16"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image):\n",
    "    image = preprocess_image_data(image)\n",
    "    temp = model_new.predict(image)\n",
    "    temp = np.reshape(temp, temp.shape[1])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 6000/6000 [10:22<00:00,  9.64it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_encode = {}\n",
    "for imag in tqdm(train):\n",
    "    train_data_encode[imag[len(file_location_images):]] = encode_image(imag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/pickel_store/encoded_images_train.p\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(train_data_encode, encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1000/1000 [02:04<00:00,  8.02it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data_encode = {}\n",
    "for img in tqdm(test):\n",
    "    test_data_encode[img[len(file_location_images):]] = encode_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/pickel_store/encoded_images_test.p\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(test_data_encode, encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((2048,), (2048,))"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "train_data_encode[train[0][len(file_location_images):]].shape, test_data_encode[test[0][len(file_location_images):]].shape"
   ]
  },
  {
   "source": [
    "len(test_data_encode), len(train_data_encode)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 6000)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = {}\n",
    "for i in train:\n",
    "    if i[len(file_location_images):] in dictionary_captions:\n",
    "        train_d[i] = dictionary_captions[i[len(file_location_images):]]\n",
    "\n",
    "val_d = {}\n",
    "for j in val:\n",
    "    if j[len(file_location_images):] in dictionary_captions:\n",
    "        val_d[j] = dictionary_captions[j[len(file_location_images):]]\n",
    "\n",
    "test_d = {}\n",
    "for k in test:\n",
    "    if k[len(file_location_images):] in dictionary_captions:\n",
    "        test_d[k] = dictionary_captions[k[len(file_location_images):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6000, 1000, 1000)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "len(train_d), len(val_d), len(test_d)"
   ]
  },
  {
   "source": [
    "# CALCULATING UNIQUE WORDS IN CAPTIONS, MAPPING UNIQUE WORDS TO INDICES AND VICE-VERSA IN 2 SEPERATE DICTIONARIES, MAX_LENGTH OF CAPTIONS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps = []\n",
    "for key, value in train_d.items():\n",
    "    for i in value:\n",
    "        caps.append('<start> ' + i + ' <end>')\n",
    "\n",
    "cap_words = [i.split() for i in caps]\n",
    "\n",
    "unique_captions = []\n",
    "for i in cap_words:\n",
    "    unique_captions.extend(i)\n",
    "\n",
    "unique_captions = list(set(unique_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/pickel_store/unique_captions.p\", \"wb\") as i:\n",
    "    pickle.dump(unique_captions, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8256"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "unique = pickle.load(open(\"/Users/vishaljha/Documents/GitHub/ML Playground/ImageCaptioning/Model/pickel_store/unique_captions.p\", \"rb\"))\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {val:index for index, val in enumerate(unique)}\n",
    "idx2word = {index:val for index, val in enumerate(unique)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4776, 'classes')"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "word2idx['<start>'], idx2word[3482]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cap_length = 0\n",
    "for c in caps:\n",
    "    c = c.split()\n",
    "    if len(c) > max_cap_length:\n",
    "        max_cap_length = len(c)\n",
    "\n",
    "vocab_size = len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(40, 8256)"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "max_cap_length, vocab_size"
   ]
  },
  {
   "source": [
    "# ADDING < START> AND < END> TAGS TO INDICATE STARTING AND ENDING OF A SENTENCE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "f = open(file_location_training_data, 'w')\n",
    "f.write(\"image_id\\tcaptions\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in train_d.items():\n",
    "    for i in value:\n",
    "        f.write(key[len(file_location_images):] + '\\t' + '<start> ' + i + ' <end>' + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(30000, 30000)"
      ]
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "source": [
    "df = pd.read_csv(file_location_training_data, delimiter='\\t')\n",
    "c = [i for i in df['captions']]\n",
    "\n",
    "len(df), len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('<start> A longeared dog running in the grass . <end>',\n",
       " '2764732789_1392e962d0.jpg')"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "imgs = [i for i in df['image_id']]\n",
    "\n",
    "a = c[-1]\n",
    "a, imgs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "383454"
      ]
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "source": [
    "sample_per_epochs = 0\n",
    "for ca in caps:\n",
    "    sample_per_epochs += len(ca.split()) - 1\n",
    "\n",
    "sample_per_epochs"
   ]
  },
  {
   "source": [
    "# CAPTION GENERATOR"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size= 64):\n",
    "    partial_captions = []\n",
    "    next_word = []\n",
    "    images= []\n",
    "\n",
    "    df = pd.read_csv(file_location_training_data, delimiter='\\t')\n",
    "    df = df.sample(frac=1)\n",
    "    iter = df.iterrows()\n",
    "\n",
    "    c = []\n",
    "    imgs = []\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        x = next(iter)\n",
    "        c.append(x[1][1])\n",
    "        imgs.append(x[1][0])\n",
    "\n",
    "    count = 0\n",
    "    while True:\n",
    "        for idx, text in enumerate(c):\n",
    "            current_image = train_data_encode[imgs[idx]]\n",
    "\n",
    "            for i in range(len(text.split()) - 1):\n",
    "                count += 1\n",
    "\n",
    "                partial = [word2idx[txt] for txt in text.split()[:i+1]]\n",
    "                partial_captions.append(partial)\n",
    "\n",
    "                n = np.zeros(vocab_size)\n",
    "                n[word2idx[text.split()[i+1]]] = 1\n",
    "                next_word.append(n)\n",
    "\n",
    "                images.append(current_image)\n",
    "\n",
    "                if count >= batch_size:\n",
    "                    next_word = np.asarray(next_word)\n",
    "                    images = np.asarray(images)\n",
    "                    partial_captions = sequence.pad_sequences(partial_captions, maxlen=max_cap_length, padding='post')\n",
    "                    yield[[images, partial_captions], next_word]\n",
    "\n",
    "                    partial_captions = []\n",
    "                    next_word = []\n",
    "                    images = []\n",
    "                    count = 0"
   ]
  }
 ]
}